Mathematical Foundations of the Naive Bayes Rating Classifier
This document outlines the mathematical foundation of a Naive Bayes classifier used for rating prediction in a review system. The classifier assigns ratings (e.g., 1 to 5 stars) to reviews based on the likelihood of words appearing in each rating class, applied independently to categories such as seat comfort or food quality.
1. Naive Bayes Theorem
The Naive Bayes classifier is based on Bayes' Theorem, which computes the posterior probability of a rating class ( r ) given words ( w_1, w_2, \ldots, w_n ):
[P(r | w_1, w_2, \ldots, w_n) = \frac{P(r) \cdot P(w_1, w_2, \ldots, w_n | r)}{P(w_1, w_2, \ldots, w_n)}]
Since ( P(w_1, w_2, \ldots, w_n) ) is constant across all rating classes ( r ), we use the proportional form for classification:
[P(r | w_1, w_2, \ldots, w_n) \propto P(r) \cdot P(w_1, w_2, \ldots, w_n | r)]
2. Conditional Independence Assumption
The "naive" assumption is that words are conditionally independent given the rating class ( r ). Thus, the joint probability is:
[P(w_1, w_2, \ldots, w_n | r) = \prod_{i=1}^{n} P(w_i | r)]
Substituting this into the posterior probability:
[P(r | w_1, w_2, \ldots, w_n) \propto P(r) \cdot \prod_{i=1}^{n} P(w_i | r)]
This assumption simplifies computation but may not fully capture word dependencies in natural language.
3. Logarithmic Transformation
To avoid numerical underflow from multiplying small probabilities and to improve computational efficiency, we apply a logarithmic transformation:
[\log P(r | w_1, \ldots, w_n) \propto \log P(r) + \sum_{i=1}^{n} \log P(w_i | r)]
This converts the product into a sum, ensuring numerical stability.
4. Parameter Estimation
4.1 Prior Probability ( P(r) )
The prior probability of a rating class ( r ) is estimated from the training data:
[P(r) = \frac{\text{Number of reviews with rating } r}{\text{Total number of reviews}}]
4.2 Likelihood ( P(w | r) )
The likelihood of a word ( w ) given a rating class ( r ) is computed with Laplace smoothing to handle unseen words:
[P(w | r) = \frac{\text{Count of word } w \text{ in reviews with rating } r + 1}{\text{Total number of words in rating class } r + |V|}]
Where ( |V| ) is the size of the vocabulary (the number of unique words across all training reviews in a category).
5. Prediction
The classifier predicts the rating ( r^* ) that maximizes the log-posterior probability:
[r^* = \arg\max_{r} \left[ \log P(r) + \sum_{i=1}^{n} \log P(w_i | r) \right]]
For unseen words in test reviews, a small probability (e.g., ( \frac{1}{\text{Total number of words in rating } r + |V|} )) is assigned using Laplace smoothing.
6. Implementation Notes

Category-Specific Training: A separate classifier is trained for each category (e.g., seat comfort, food quality) using relevant reviews.
Preprocessing:
Convert text to lowercase.
Remove punctuation.
Filter out common stopwords (e.g., "the", "a", "and").
Tokenize text into words (splitting on whitespace).


Prediction Process: For each review, compute the log-probability for each rating class and select the rating with the highest score.
Handling Unseen Words: Words not in the training vocabulary are assigned a smoothed probability or ignored, depending on implementation.

7. Smoothing and Vocabulary

Laplace Smoothing: Adds 1 to word counts to prevent zero probabilities for unseen words, ensuring robustness.
Vocabulary: Built from all unique words in the training reviews for a category, shared across all rating classes within that category率先

category.
